{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ucvm7IH6da3"
      },
      "source": [
        "# APS1070\n",
        "#### Tutorial 2 - Anomaly Detection Algorithm using Gaussian Mixture Model \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGO4OT_Ce8Sj"
      },
      "source": [
        "In this part of tutorial, we will implement an anomaly detection algorithm using the Gaussian model to detect anomalous behavior in a 2D dataset first and then a high-dimensional dataset.\n",
        "\n",
        "Loading relevant libraries and the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTx2BdY3e8S8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "X, y_true = make_blobs(n_samples=400, centers=1,\n",
        "                       cluster_std=0.60, random_state=0)\n",
        "X_append, y_true_append = make_blobs(n_samples=10,centers=1,\n",
        "                                    cluster_std=5,random_state=0)\n",
        "X = np.vstack([X,X_append])\n",
        "y_true = np.hstack([y_true, [1 for _ in y_true_append]])\n",
        "X = X[:, ::-1] # flip axes for better plotting\n",
        "plt.scatter(X[:,0],X[:,1],marker=\"x\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mD8qOcC2e8Te"
      },
      "source": [
        "Here we've manufactured a dataset where some points are visibly outliers from the main distribution.\n",
        "\n",
        "We can see this from looking at the plot, but how do we robustly identify the outliers? \n",
        "\n",
        "That's where a Gaussian estimation comes in. For this dataset, we only need a single Gaussian, for which we are gonna calculate the mean and standard deviation. Then, we're able to find the points that don't seem likely to have originated from that distribution - these are our outliers!\n",
        "\n",
        "First, we need to calculate the mean and variance for our data. Complete the function below to generate these values using these formulas:\n",
        "\n",
        "$$\\mu = \\frac{1}{m} \\sum_{i=1}^{m}X_i$$\n",
        "\n",
        "$$\\sigma^2 = \\frac{1}{m} \\sum_{i=1}^{m}(X_i-\\mu)^2$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVGj1_q2DyXX"
      },
      "outputs": [],
      "source": [
        "def estimateGaussian(X):\n",
        "    \"\"\"\n",
        "     This function estimates the parameters of a Gaussian distribution using the data in X\n",
        "    \"\"\"\n",
        "    \n",
        "    m = X.shape[0]\n",
        "    \n",
        "    #compute mean of X\n",
        "    sum_ = np.sum(X,axis=0)\n",
        "    mu = 1/m *sum_\n",
        "    \n",
        "    # compute variance of X\n",
        "    var = 1/m * np.sum((X - mu)**2,axis=0)\n",
        "    \n",
        "    return mu,var\n",
        "mu, sigma2 = estimateGaussian(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ulJhFFHe8T5"
      },
      "source": [
        "Now, we will calculate for each point in X, the probability of the distribution $N(\\mu,\\sigma^2)$ generating that point randomly. This has been completed for you, although it is important to understand how the calculation of the PDF works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdvju5Ive8UF"
      },
      "outputs": [],
      "source": [
        "def multivariateGaussian(X, mu, sigma2):\n",
        "    \"\"\"\n",
        "    Computes the probability density function of the multivariate gaussian distribution.\n",
        "    \"\"\"\n",
        "    k = len(mu)\n",
        "    \n",
        "    sigma2=np.diag(sigma2)\n",
        "    X = X - mu.T\n",
        "    p = 1/((2*np.pi)**(k/2)*(np.linalg.det(sigma2)**0.5))* np.exp(-0.5* np.sum(X @ np.linalg.pinv(sigma2) * X,axis=1))\n",
        "    return p\n",
        "p = multivariateGaussian(X, mu, sigma2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_Sze60ie8Uc"
      },
      "source": [
        "Now that we have the probability of each point in the dataset, we can plot these on the original scatterplot:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POrlps2qe8Uj"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(X[:,0],X[:,1],marker=\"x\",c=p,cmap='viridis');\n",
        "plt.colorbar();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3eLSr5Ae8U4"
      },
      "source": [
        "We're getting closer to the point where we can programmatically  identify our outliers for a single Gaussian distribution. The last step is to identify a value for $p$, below which we consider a point to be an outlier. We term this $\\epsilon$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1snRc3pbe8VN"
      },
      "outputs": [],
      "source": [
        "#Choose a value for Threshold\n",
        "print (p.min() , p.max())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ig022haSdKjO"
      },
      "outputs": [],
      "source": [
        "_ = plt.hist (p, bins = 500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXZPS_eWdawO"
      },
      "outputs": [],
      "source": [
        "tr = 0.01"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbkVvmuBe8Wb"
      },
      "source": [
        "Now we'll highlight on the scatter plot all points that are below $\\epsilon$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jm-ifxxQe8Wn"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(X[:,0],X[:,1],marker=\"x\",c=p,cmap='viridis');\n",
        "# Circling of anomalies\n",
        "outliers = np.nonzero(p<tr)[0]\n",
        "plt.scatter(X[outliers,0],X[outliers,1],marker=\"o\",facecolor=\"none\",edgecolor=\"r\",s=70);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOxDRW3ce8XA"
      },
      "source": [
        "Try different values for threshold and report Precision, Recall and F1 Score. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JcEkhk2e8XJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "tr_list = [0.001, 0.01, 0.02, 0.03, 0.1 , 0.2 , 1]\n",
        "for tr in tr_list:\n",
        "    pr = precision_score(y_true, p<=tr)\n",
        "    rc = recall_score(y_true, p<=tr)\n",
        "    f1 = f1_score(y_true, p<=tr)\n",
        "    print (\"Threshold:\" , \"{:.3f}\".format(tr), \" Precision:\" , \"{:.2f}\".format(pr), \" Recall: \" , \"{:.2f}\".format(rc) , \" F1 Score: \", \"{:.2f}\".format(f1))\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUUfRXXUe8Xg"
      },
      "source": [
        "You may have noticed that in this example, we are training and testing on the _entire_ dataset. This is absolutely not standard practice! You should _always_ split into a training and testing set. However, the reason that we can get away with this here is that we don't actually use labels at all during training - this is an _unsupervised_ machine learning task. Unsupervised learning methods are beneficial for anomaly detection because in the real world (i.e. testing!) we might come across types of outliers that we didn't see during training. We want to use a method that can handle this, and unsupervised methods are often better suited to this type of domain.\n",
        "\n",
        "For the next section, we'll move to a Mixture of Gaussian models. Take a look at the following dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "un2Y2Ci3e8Xp"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.model_selection import train_test_split\n",
        "X, y_true = make_blobs(n_samples=400, centers=5,\n",
        "                       cluster_std=0.60, random_state=1)\n",
        "X_append, y_true_append = make_blobs(n_samples=50,centers=5,\n",
        "                                    cluster_std=5,random_state=1)\n",
        "X = np.vstack([X,X_append])\n",
        "y_true = np.hstack([[0 for _ in y_true], [1 for _ in y_true_append]])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_true, test_size=0.33, random_state=1, shuffle=True)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(X_train[:,0],X_train[:,1],marker=\"x\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fs6_y8BFe8Yb"
      },
      "source": [
        "Okay, we have more than one cluster centre now. So what? Let's just ignore that and use the same model as before. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJ0Y6MUmUrfx"
      },
      "outputs": [],
      "source": [
        "mu, sigma2 = estimateGaussian(X_train)\n",
        "p = multivariateGaussian(X_train, mu, sigma2)\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(X_train[:,0],X_train[:,1],marker=\"x\",c=p,cmap='viridis');\n",
        "outliers = np.nonzero(p<0.001)[0]\n",
        "plt.scatter(X_train[outliers,0],X_train[outliers,1],marker=\"o\",facecolor=\"none\",edgecolor=\"r\",s=70);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-j_1On8e8Yi"
      },
      "outputs": [],
      "source": [
        "mu, sigma2 = estimateGaussian(X_train)\n",
        "p = multivariateGaussian(X_test, mu, sigma2)\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(X_test[:,0],X_test[:,1],marker=\"x\",c=p,cmap='viridis');\n",
        "outliers = np.nonzero(p<0.001)[0]\n",
        "plt.scatter(X_test[outliers,0],X_test[outliers,1],marker=\"o\",facecolor=\"none\",edgecolor=\"r\",s=70);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kmcR_pCe8Y6"
      },
      "source": [
        "Uh oh. This model performs terribly. It's fit the mean to a section of space where we don't have _any_ points, and it has absolutely no idea which points are outliers! This was probably pretty obvious to you though. We need to move to a Mixture of Gaussians model - one in which we use multiple Gaussians to fit the data. We'll use `sklearn.mixture.GaussianMixture` to do this - or rather you will! Use the documentation, found [here](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture) to initialise and fit a `GaussianMixture` object called `gm` in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOcXDk-ie8ZD"
      },
      "outputs": [],
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "gm = GaussianMixture(n_components = 5,\n",
        "                    covariance_type = 'full', random_state=0, )\n",
        "gm.fit(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1vVv9cHEqkd"
      },
      "source": [
        "Now we can use the method `gm.score_samples()` which gives a score based on how likely a point is to have been generated by any cluster:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qftoq8ji9Ugi"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "c = gm.score_samples(X_train)\n",
        "plt.scatter(X_train[:,0],X_train[:,1],c=gm.score_samples(X_train),cmap='viridis',marker='x')\n",
        "\n",
        "threshold= -4.8\n",
        "\n",
        "outliers = np.nonzero(c<threshold)[0]\n",
        "plt.scatter(X_train[outliers,0],X_train[outliers,1],marker=\"o\",facecolor= \"none\",edgecolor=\"r\",s=70)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZ0g9Suie8Ze"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "c = gm.score_samples(X_test)\n",
        "plt.scatter(X_test[:,0],X_test[:,1],c=gm.score_samples(X_test),cmap='viridis',marker='x')\n",
        "\n",
        "threshold= -4.8\n",
        "\n",
        "outliers = np.nonzero(c<threshold)[0]\n",
        "plt.scatter(X_test[outliers,0],X_test[outliers,1],marker=\"o\",facecolor= \"none\",edgecolor=\"r\",s=70)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UNHXYGQE051"
      },
      "source": [
        "What is the variable \"`threshold`\"? ______________ Why is it negative? __________________________\n",
        " \n",
        "Now we can use the method `gm.predict_proba()` to spot the points in each of the clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPOHN1zwE4hN"
      },
      "outputs": [],
      "source": [
        "Non_Outliers=np.nonzero(c>=threshold)[0]\n",
        "X_t=X_train[Non_Outliers]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pupN5PmuE67x"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "for i in range(5):\n",
        "    plt.subplot(3,2,i+1)\n",
        "    plt.scatter(X_t[:,0],X_t[:,1],c=gm.predict_proba(X_t)[:,i],cmap='viridis',marker='x')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4A4pD4jE9kb"
      },
      "source": [
        "1] What do functions `gm.score_samples()` and `gm.predict_proba()` return? ___________ \n",
        "\n",
        "2] Why it was important to run them in above sequence? ____________________\n",
        "\n",
        "3] What is the difference between the two function?_______________________________\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ue-8F_8_e8Zp"
      },
      "source": [
        "Our Mixture of Gaussians model is powerful! Not only is it unsupervised, it can both classify points into one of the K clusters we have, _and_ it can help us with our ultimate goal of identifying outlier points! We can do this by finding the points that no cluster wants to claim for itself.\n",
        "\n",
        "In the cells below, be are going to try two methods to determine the threshold and visit ROC and AUC. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSZFlYcJgDlH"
      },
      "outputs": [],
      "source": [
        "p_gm = gm.score_samples(X_train)\n",
        "_ = plt.hist (p_gm , bins = 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNXCnzife8Zt"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "\n",
        "p_gm = gm.score_samples(X_train) #score_samples will compute the weighted log probabilities for each sample\n",
        "\n",
        "print (\"Method 1: \")\n",
        "\n",
        "for tr in [  -20, -10 ,-7, -6 ,  -5 ]:\n",
        "   precision = precision_score(y_train, p_gm < tr) #Here, we compare y_train labels to our picks using precision\n",
        "   recall = recall_score(y_train, p_gm < tr) #Here, we compare y_train labels to our picks using recall\n",
        "   print('For threshold of ',tr,' \\t precision: ', '%.3f' % precision,' recall: ', '%.3f' % recall) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print (\"\\n\\nMethod 2: \")\n",
        "\n",
        "for i in [1, 10, 20, 30, 100 , 120, 130]: \n",
        "   tr = sorted(p_gm)[i] #We sort the points by probability\n",
        "   precision = precision_score(y_train, p_gm < tr) \n",
        "   recall = recall_score(y_train, p_gm < tr) \n",
        "   print('For k: ',i,'\\t  threshold: ','%.3f'% tr ,'  precision: ', '%.3f' % precision,'  recall: ', '%.3f' % recall) \n",
        "\n",
        "\n",
        "# NEXT: find the best threshold and try the model on the test set!\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#This part computes the ROC curves for both models like we talked about in class\n",
        "\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "gm_sf = GaussianMixture(n_components = 1, covariance_type = 'full', random_state=0, )\n",
        "gm_sf.fit(X_train)\n",
        "p = gm_sf.score_samples(X_train)\n",
        "\n",
        "fpr_sc, tpr_sc, _ = roc_curve(y_train, -1 * p)\n",
        "fpr_gm, tpr_gm, _ = roc_curve(y_train, -1 * p_gm)\n",
        "plt.plot(fpr_sc, tpr_sc, linestyle = '--', label='Single Component')\n",
        "plt.plot(fpr_gm, tpr_gm, marker='.', label='Gaussian Mixture')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wnXMWlaukzNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhqZHsEgau_-"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import auc\n",
        "print (\"AUC of Single Component\" , format( auc(fpr_sc, tpr_sc) , \".3f\"))\n",
        "print (\"AUC of Gaussian Mixture\" , format( auc(fpr_gm, tpr_gm) , \".3f\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulj6N3ytcnrU"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "print (\"AUC of Single Component\" , format(  roc_auc_score(y_train, -1 * p)     , \".3f\")  )\n",
        "print (\"AUC of Gaussian Mixture\" , format(  roc_auc_score(y_train, -1 * p_gm)  , \".3f\")  )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#This part computes the ROC curves for both models like we talked about in class\n",
        "\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "gm2 = GaussianMixture(n_components = 10, covariance_type = 'full', random_state=0, )\n",
        "gm2.fit(X_train)\n",
        "p_gm2 = gm2.score_samples(X_train)\n",
        "\n",
        "fpr_sc, tpr_sc, _ = roc_curve(y_train, -1 * p)\n",
        "fpr_gm, tpr_gm, _ = roc_curve(y_train, -1 * p_gm)\n",
        "fpr_gm2, tpr_gm2, _ = roc_curve(y_train, -1 * p_gm2)\n",
        "plt.plot(fpr_sc, tpr_sc, linestyle = '--', label='Single Component')\n",
        "plt.plot(fpr_gm, tpr_gm, marker='.', label='Gaussian Mix 5 cmp')\n",
        "plt.plot(fpr_gm2, tpr_gm2, marker='*', label='Gaussian Mix 10 cmp')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jL6jZgqCnt5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ft37989Ze8Z4"
      },
      "source": [
        "Let's look at a dataset that motivates using a Mixture of Gaussians model: Simpsons ratings.\n",
        "\n",
        "Everyone knows that there's a certain point when The Simpsons \"got bad\", but can we use a Mixture of Gaussians to find out exactly when that was?\n",
        "\n",
        "Load up the `simpsons.pickle` file using the cell below. It contains the IMDb rating for every simpsons episode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZoowFfVve8Z7"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/alexwolson/APS1070_data/raw/master/simpsons.pickle\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.mixture import GaussianMixture\n",
        "with open('simpsons.pickle','rb') as f:\n",
        "    simpsons = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vwlvtV9e8ad"
      },
      "source": [
        "Plot a histogram of the rating distribution for all Simpsons episodes. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxZC72Wme8al"
      },
      "outputs": [],
      "source": [
        "allratings = []\n",
        "for v in simpsons.values():\n",
        "    for v1 in v.values():\n",
        "        allratings.append(v1)\n",
        "plt.hist(allratings, bins= 30)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yD3dnH7Ce8at"
      },
      "source": [
        "Next, use Gaussian Mixture to fit a Mixture of Gaussians to the Simpsons rating distribution. Since we are trying to distinguish between good and bad ratings, we only need 2 gaussians.\n",
        "\n",
        "* What are the means for the two Gaussians fit by the model? __\n",
        "* What about the standard deviations? __"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmJ5ESd7e8ay"
      },
      "outputs": [],
      "source": [
        "gm = GaussianMixture(n_components=2)\n",
        "gm.fit(np.array(allratings).reshape(-1,1))\n",
        "\n",
        "print(\"Mean: \", gm.means_)\n",
        "print(\" \\n \\n StD: \", np.sqrt(gm.covariances_))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjPbkEjDi0FC"
      },
      "outputs": [],
      "source": [
        "from matplotlib import rc\n",
        "from sklearn import mixture\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.ticker as tkr\n",
        "import scipy.stats as stats\n",
        "\n",
        "\n",
        "x = np.array(allratings)\n",
        "\n",
        "f = np.ravel(x).astype(np.float)\n",
        "f=f.reshape(-1,1)\n",
        "\n",
        "weights = gm.weights_\n",
        "means = gm.means_\n",
        "covars = gm.covariances_\n",
        "\n",
        "plt.hist(f, bins=30, histtype='bar', density=True, ec='blue', alpha=0.5)\n",
        "\n",
        "f_axis = f.copy().ravel()\n",
        "f_axis.sort()\n",
        "plt.plot(f_axis,weights[0]*stats.norm.pdf(f_axis,means[0],np.sqrt(covars[0])).ravel(), c='red', label = \"G1\")\n",
        "plt.plot(f_axis,weights[1]*stats.norm.pdf(f_axis,means[1],np.sqrt(covars[1])).ravel(), c='blue', label = \"G2\")\n",
        "plt.legend()\n",
        "plt.xlabel(\"Rate\")\n",
        "plt.ylabel(\"Dist\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFf8OR1me8a8"
      },
      "source": [
        "Finally, using the `GuassianMixture.predict()` method, we can use maximum likelihood to estimate which distribution, good or bad, each episode belongs to. In the cell below, we have provided code to count the number of episodes predicted to be in the \"good\" distribution per season, and plot for the same.\n",
        "Understand the code and answer the question.\n",
        "\n",
        "* Where is the notable drop-off point? __\n",
        "* What is the first season with 0 good episodes? __\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETyADy8xY0kM"
      },
      "outputs": [],
      "source": [
        "#Let's first associate each component with a good or bad season\n",
        "if gm.means_[0,0] > gm.means_[1,0]: #True if first component is the good season (ie, higher mean)\n",
        "     Good_season_index = 0\n",
        "else:\n",
        "     Good_season_index = 1\n",
        "\n",
        "Xs = []\n",
        "Ys = []\n",
        "simpsons = dict(sorted(list(simpsons.items()), key=lambda x: x[0]))\n",
        "for season, episodes in simpsons.items():\n",
        "     bad = 0\n",
        "     good = 0\n",
        "     for episode in episodes.values():\n",
        "          if gm.predict(np.array(episode).reshape(-1,1)) == Good_season_index:\n",
        "               good += 1\n",
        "          else:\n",
        "               bad += 1\n",
        "     Xs.append(season)\n",
        "     Ys.append(good/(good+bad))\n",
        "plt.plot(Xs, Ys, lw = 4, color = \"green\");\n",
        "plt.xlabel(\"Seasons\")\n",
        "plt.ylabel(\"Good ratio\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "W22_APS1070_Tutorial_2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}