{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "W22_APS1070_Tutorial_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOh961MbT2RV"
      },
      "source": [
        "# APS1070 \n",
        "#### Basic Principles and Models - Tutorial 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icBmriRPT2RX"
      },
      "source": [
        "In this tutorial, we will be using the popular machine learning library [scikit-learn](https://scikit-learn.org/stable/) in tandem with a popular scientific computing library in Python, [NumPy](https://www.numpy.org/), to investigate basic machine learning principles and models. The topics that will be covered in this lab include:\n",
        "* Introduction to scikit-learn and NumPy\n",
        "* Data preparation and cleaning with Pandas\n",
        "* Exploratory data analysis (EDA)\n",
        "* Nearest neighbors classification algorithm\n",
        "* Nested cross-validation\n",
        "\n",
        "*Note:* Some other useful Python libraries include [matplotlib](https://matplotlib.org/) (for plotting/graphing) and [Pandas](https://pandas.pydata.org/) (for data analysis), though we won't be going into detail on these in this bootcamp. \n",
        "\n",
        "##### Jupyter Notebooks\n",
        "This lab will be using [Jupyter Notebooks](https://jupyter.org/) as a Python development environment. Hopefully you're somewhat familiar with them. Write your code in *cells* (this is a cell!) and execute your code by pressing the *play* button (up top) or by entering *ctrl+enter*. To format a cell for text, you can select \"Markdown\" from the dropdown - the default formatting is \"Code\", which will usually be what you want.\n",
        "\n",
        "#### Getting started\n",
        "Let's get started. First, we're going to test that we're able to import the required libraries.  \n",
        "**>> Run the code in the next cell** to import scikit-learn,NumPy, and Pandas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHKEA-_aT2RZ"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICUQIY-oT2Rg"
      },
      "source": [
        "### NumPy Basics\n",
        "\n",
        "Great. Let's move on to our next topic: getting a handle on NumPy basics. You can think of NumPy as sort of like a MATLAB for Python (if that helps). The main object is multidimensional arrays, and these come in particularly handy when working with data and machine learning algorithms.\n",
        "\n",
        "Let's create a 2x4 array containing the numbers 1 through 8 and conduct some basic operations on it.  \n",
        "**>> Run the code in the next cell to create and print the array.***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Trpi4fApz7ud"
      },
      "source": [
        "# see all functions and attributes in numpy\n",
        "print(dir(np))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zBy37aX6XAg"
      },
      "source": [
        "print(help(np.arange))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9Fr13p7T2Rp"
      },
      "source": [
        "We can access the shape, number of dimensions, data type, and number of elements in our array as follows:  \n",
        "*(Tip: use \"print()\" when you want a cell to output more than one thing, or you want to append text to your output, otherwise the cell will output the last object you call, as in the cell above)*"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "array = np.arange(8).reshape(2,4)\n",
        "array"
      ],
      "metadata": {
        "id": "hICXfMfknJUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koG8G0dQT2Rq"
      },
      "source": [
        "print (\"Shape:\", array.shape)\n",
        "print (\"Dimensions:\", array.ndim)\n",
        "print (\"Data type:\" , array.dtype.name)\n",
        "print (\"Number of elements:\", array.size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWcULO4UT2Ry"
      },
      "source": [
        "If we have a Python list containing a set of numbers, we can use it to create an array:  \n",
        "*(Tip: if you click on a function call, such as array(), and press \"shift+tab\" the Notebook will provide you all the details of the function)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWCuD9tGT2R0"
      },
      "source": [
        "mylist = [0, 1, 1, 2, 3, 5, 8, 13, 21]\n",
        "myarray = np.array(mylist)\n",
        "myarray"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0CFN-IeT2R7"
      },
      "source": [
        "And we can do it for nested lists as well, creating multidimensional NumPy arrays:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4f24eQ4wT2R9"
      },
      "source": [
        "my2dlist = [[1,2,3],[4,5,6]]\n",
        "my2darray = np.array(my2dlist)\n",
        "my2darray"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOsAaWCGT2SE"
      },
      "source": [
        "We can also index and slice NumPy arrays like we would do with a Python list or another container object as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCZeXoCYT2SF"
      },
      "source": [
        "array = np.arange(10)\n",
        "print (\"Originally: \", array)\n",
        "print (\"First four elements: \", array[:4])\n",
        "print (\"After the first four elements: \", array[4:])\n",
        "print (\"Elements 3 to 7: \", array[3:8]) \n",
        "print (\"The last element: \", array[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hj3v49acT2SO"
      },
      "source": [
        "And we can index/slice multidimensional arrays, too."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9KiwpGFT2SQ"
      },
      "source": [
        "array = np.array([[1,2,3],[4,5,6]])\n",
        "print (\"Originally: \", array)\n",
        "print (\"First row only: \", array[0])\n",
        "print (\"First column only: \", array[:,0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZnePSB2T2SW"
      },
      "source": [
        "#### Sneak preview\n",
        "\n",
        "Often, when designing a machine learning classifier, it can be useful to compare an array of predictions (0 or 1 values) to another array of true values. We can do this pretty easily in NumPy to compute the *accuracy* (e.g., the number of values that are the same), for example, as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEJzV878T2SY"
      },
      "source": [
        "true_values = [0, 0, 1, 1, 1, 1, 1, 0, 1, 0]\n",
        "predictions = [0, 0, 0, 1, 1, 1, 0, 1, 1, 0]\n",
        "\n",
        "true_values_array = np.array(true_values)\n",
        "predictions_array = np.array(predictions)\n",
        "\n",
        "accuracy = np.sum(true_values_array == predictions_array) / true_values_array.size\n",
        "print (\"Accuracy: \", accuracy * 100, \"%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvfZTvSzT2Se"
      },
      "source": [
        "In the previous cell, we took two Python lists, converted them to NumPy arrays, and then used a combination of np.sum() and .size to compute the accuracy (proportion of elements that are pairwise equal). A tiny bit more advanced, but demonstrates the power of NumPy arrays.\n",
        "\n",
        "You'll notice we didn't use nested loops to conduct the comparison, but instead used the np.sum() function. This is an example of a vectorized operation within NumPy that is much more efficient when dealing with large datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35d_UGIINRRQ"
      },
      "source": [
        "Numpy contains a wide range of mathematical functions. You can use the following to see a list of mathematical functions supported by numpy: https://numpy.org/doc/stable/reference/routines.math.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nLWoZex955E"
      },
      "source": [
        "# mathematical functions\n",
        "array1 = np.arange(9).reshape(3,3)\n",
        "array2= np.arange(5,14).reshape(3,3)\n",
        "print('array1:',array1)\n",
        "\n",
        "print('array2:',array2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMJR78JyPh_6"
      },
      "source": [
        "# summation over an axis\n",
        "np.sum(array1,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oqk781LEPlaY"
      },
      "source": [
        "# adding arrays \n",
        "array1+array2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJ7mpPOQQqtP"
      },
      "source": [
        "# dot product\n",
        "array1@array2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOhIyI3hT2Sf"
      },
      "source": [
        "### Pandas basics\n",
        "\n",
        "Pandas is an incredibly useful library that allows us to work with large datasets in Python. It contains myriad useful tools, and is highly compatible with other libraries like Scikit-learn, so you don't have to spend any time getting the two to play nicely together.\n",
        "\n",
        "First we are going to load a dataset with Pandas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LGc761ZT2Sg"
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "to0gPM-uT2So"
      },
      "source": [
        "import wget\n",
        "\n",
        "wget.download(\n",
        "    'https://raw.githubusercontent.com/aps1070-2019/datasets/master/arabica_data.csv',\n",
        "    'arabica_data.csv'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5qBfMoHT2Su"
      },
      "source": [
        "df = pd.read_csv('arabica_data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNDAuNdrT2S0"
      },
      "source": [
        "With Pandas, the main object we work with is referred to as a _DataFrame_ (hence calling our object here df). A DataFrame stores our dataset in a way that immediately gives us a lot of power to interact with it. If you just put the DataFrame in a cell on its own, you instantly get a clear, easy to read preview of the data you have:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46qskYKmT2S1"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evWv59w_X_fz"
      },
      "source": [
        "# see first 5 rows\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2Age2nfYJFT"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Nd-qnZyT2S6"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vE7IE1LdHi1"
      },
      "source": [
        "# see the data types in each column\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsSfu06QT2S7"
      },
      "source": [
        "# getting the summary of numerical columns\n",
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GilAJMK4T2TB"
      },
      "source": [
        "Let's say we want to zero in on a single column. This is done the same way that you access a dictionary entry:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZbJm53OT2TD"
      },
      "source": [
        "df['Species']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsII7MuRT2TK"
      },
      "source": [
        "Using this method of column access on its own returns a `series` object - think of this as a DataFrame with only one column. If you want to get the raw values however, you can simply specify this by adding `.values` after your entry. Using this, and by putting the object in a `Set` (which does not allow duplicate entries), we can quickly see all of the possible values for any column:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tzby9cAbT2TN"
      },
      "source": [
        "set(df['Variety'].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXorCk1fT2TS"
      },
      "source": [
        "You may notice that the final entry in this set isn't like the others - it's `nan`, which in Pandas denotes a missing entry. When working with real world datasets it's very common for entries to be missing, and there are a variety of ways of approaching a problem like this. For now, though, we are simply going to tell Pandas to drop any row that has a missing column, using the `dropna()` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Um8Rem80T2TS"
      },
      "source": [
        "df_clean = df.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m52qPIrOEYJ8"
      },
      "source": [
        "df.shape,df_clean.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJt2rnrDT2TY"
      },
      "source": [
        "* What percentage of entries are left in `df_clean`? \n",
        "* What column had the highest number of `nan` entries?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJUhACuUiOdC"
      },
      "source": [
        "df.isna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nM4SzGVaT2TZ"
      },
      "source": [
        "# What percentage of entries are left in `df_clean`? \n",
        "print(\"%.2f%% entries from full dataset are left in df_clean\" % ((len(df_clean)/len(df))*100))\n",
        "\n",
        "# What column had the highest number of `nan` entries? \n",
        "Nan_Entries = df.isna()\n",
        "Sum_of_Nan= Nan_Entries.sum()\n",
        "Sorted_Sum = Sum_of_Nan.sort_values()\n",
        "#print(Sorted_Sum)\n",
        "\n",
        "### Now write it in one line!\n",
        "df.isna().sum().sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lo_KsqpXT2Te"
      },
      "source": [
        "As you perform this analysis, you will probably notice that we've lost _quite a bit_ of our original data by simply dropping the `nan` values. There is another approach that we can examine, however. Instead of dropping the missing entries entirely, we can _impute_ their value using the data we do have. For a single column we can do this like so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8C-iTWaET2Te"
      },
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "imp = SimpleImputer(\n",
        "    missing_values=np.nan,\n",
        "    strategy='mean',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "imp.fit(\n",
        "    df['altitude_mean_meters'].values.reshape((-1,1)) #we have to do the reshape operation because we are only using one feature.\n",
        ")\n",
        "\n",
        "df['altitude_mean_meters_imputed'] = imp.transform(df['altitude_mean_meters'].values.reshape((-1,1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8pFlS3HT2Ti"
      },
      "source": [
        "df[['altitude_mean_meters','altitude_mean_meters_imputed']].head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0BTxb8nkRJj"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNn-EZIFT2Tn"
      },
      "source": [
        "OK, great! Now we have replaced the useless NaN values with the average height. While this obviously isn't as good as original data, in a lot of situations this can be a step up from losing rows entirely. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yOuvl-LT2Tn"
      },
      "source": [
        "Sophisticated analysis can be done in only a few lines using Pandas. Let's say that we want to get the average coffee rating by country. First, we can use the `groupby` method to automatically collect the results by country. Then, we can select the column we want - `quality_score` - and calculate its mean the same way we would using NumPy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3J5c3AhET2To"
      },
      "source": [
        "df_clean.groupby('Country of Origin')['quality_score'].mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNRP9w7jT2Tr"
      },
      "source": [
        "This is certainly interesting, but it could be presented better. First, all of the ratings are pretty high (what's the highest and lowest rating?). Let's standardize to unit mean and variance so that we can tell the difference more easily. We'll just do that on our subset here for now, but you can apply it to the entire dataset too!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8DvwFkJT2Tr"
      },
      "source": [
        "country_means = df_clean.groupby('Country of Origin')['quality_score'].mean()\n",
        "mu,si = country_means.mean(), country_means.std() #Calculate the overall mean and standard deviation of the quality scores\n",
        "country_means -= mu #Subtract the mean from every entry\n",
        "country_means /= si #Divide every entry by the standard deviation\n",
        "country_means"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOzGdbPNT2Tw"
      },
      "source": [
        "This is a lot clearer! Finally, let's sort this list so that it's easier to compare entries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "my0dmZkxT2Tw"
      },
      "source": [
        "country_means.sort_values()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwwEUFAeT2T2"
      },
      "source": [
        "Finally, we'll look at indexing using Pandas. Let's say that we want to only look at the coffee entries from Brazil. We can use the following syntax to identify those rows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tUm4EbTT2T2"
      },
      "source": [
        "df_clean[df_clean['Country of Origin'] == 'Brazil']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6vC377JT2T6"
      },
      "source": [
        "Say that out of the Brazil coffees, we only want to look at those which are the Bourbon variety. We can also chain those indexing operations like so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_gxxdv-T2T7"
      },
      "source": [
        "df_clean[df_clean['Country of Origin'] == 'Brazil'][df_clean['Variety'] == 'Bourbon']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOavsiQgT2T_"
      },
      "source": [
        "### Scikit-learn Basics\n",
        "\n",
        "Scikit-learn is a great library to use for doing machine learning in Python. Data preparation, exploratory data analysis (EDA), classification, regression, clustering; it has it all. \n",
        "\n",
        "Scikit-learn usually expects data to be in the form of a 2D matrix with dimensions *n_samples x n_features* with an additional column for the target. To get acquainted with scikit-learn, we are going to use the [iris dataset](https://archive.ics.uci.edu/ml/datasets/iris), one of the most famous datasets in pattern recognition. \n",
        "\n",
        "Each entry in the dataset represents an iris plant, and is categorized as: \n",
        "\n",
        "* Setosa (class 0)\n",
        "* Versicolor (class 1)\n",
        "* Virginica (class 2)\n",
        "\n",
        "These represent the target classes to predict. Each entry also includes a set of features, namely:\n",
        "\n",
        "* Sepal width (cm)\n",
        "* Sepal length (cm)\n",
        "* Petal length (cm)\n",
        "* Petal width (cm)\n",
        "\n",
        "In the context of machine learning classification, the remainder of the lab is going to investigate the following question:  \n",
        "*Can we design a model that, based on the iris sample features, can accurately predict the iris sample class? *\n",
        "\n",
        "Scikit-learn has a copy of the iris dataset readily importable for us. Let's grab it now and conduct some EDA."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbrEyKa-T2UA"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "iris_data = load_iris()\n",
        "feature_data = iris_data.data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7pMN0L0T2UG"
      },
      "source": [
        "* What is the shape of this feature data?\n",
        "* The data type?\n",
        "* How many samples are there? \n",
        "* How many features are there?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPWfj_zbT2UH"
      },
      "source": [
        "print(\"Feature data shape:\",feature_data.shape)\n",
        "print(\"Feature data type:\",type(feature_data[0,0]))\n",
        "print(\"Number of samples:\",feature_data.shape[0])\n",
        "print(\"Number of features:\",feature_data.shape[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZdKG_qWT2UN"
      },
      "source": [
        "Next, we will save the target classification data in a similar fashion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbv5NeXgT2UN"
      },
      "source": [
        "target_data = iris_data.target\n",
        "target_names = iris_data.target_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syYfWeqNT2UY"
      },
      "source": [
        "* What values are in \"target_data\"?\n",
        "* What is the data type?\n",
        "* What values are in \"target_names\"? \n",
        "* What is the data type?\n",
        "* How many samples are of type \"setosa\"? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEFS32ZNT2UZ"
      },
      "source": [
        "print(\"Target data content:\",np.unique(target_data))\n",
        "print(\"Target data type:\",type(target_data[0]))\n",
        "\n",
        "print(\"Target names content:\",target_names[0:3])\n",
        "print(\"Target names type:\",type(target_names[0]))\n",
        "\n",
        "setosa_samples= len([t for t in target_data if t == target_names.tolist().index('setosa')])\n",
        "print(\"%d samples of type setosa\" % setosa_samples )\n",
        "\n",
        "print(np.sum(target_data==0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efNIW-7tqCKJ"
      },
      "source": [
        "target_data==0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CM4BUd47T2Uf"
      },
      "source": [
        "We can also do some more visual EDA by plotting the samples according to a subset of the features and coloring the data points to coincide with the sample classification. We will use [matplotlib](https://matplotlib.org/), a powerful plotting library within Python, to accomplish this.\n",
        "\n",
        "For example, lets plot sepal width vs. sepal length.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdG_MYoTT2Ug"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72Io2dYoT2Um"
      },
      "source": [
        "setosa = feature_data[target_data==0]\n",
        "versicolor = feature_data[target_data==1]\n",
        "virginica = feature_data[target_data==2]\n",
        "\n",
        "plt.scatter(setosa[:,0], setosa[:,1], label=\"setosa\")\n",
        "plt.scatter(versicolor[:,0], versicolor[:,1], label=\"versicolor\")\n",
        "plt.scatter(virginica[:,0], virginica[:,1], label=\"virginica\")\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel(\"sepal length (cm)\")\n",
        "plt.ylabel(\"sepal width (cm)\")\n",
        "plt.title(\"Visual EDA\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeQYWqMGT2Us"
      },
      "source": [
        "In the above step, we used boolean indexing to filter the feature data based on the target data class. This allowed us to create a scatter plot for each of the iris classes and distinguish them by color.\n",
        "\n",
        "*Observations*: We can see that the \"setosa\" class typically consists of medium-to-high sepal width with low-to-medium sepal length, while the other two classes have lower width and higher length. The \"virginica\" class appears to have the largest combination of the two. \n",
        "\n",
        "**YOUR TURN:** \n",
        "* Which of the iris classes is seperable based on sepal characteristics? \n",
        "* Which of the iris classes is not? \n",
        "* Can we (easily) visualize each of the samples w.r.t. all features on the same plot? Why/why not? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DK4VF-zoT2Ut"
      },
      "source": [
        "### Creating a Nearest Neighbors Classifier\n",
        "\n",
        "Now that we've explored the data a little bit, we're going to use scikit-learn to create a nearest neighbors classifier for the data. Effectively we'll be developing a model whose job it is to build a relationship over input feature data (sepal and petal characteristics) that predicts the iris sample class (e.g. \"setosa\"). This is an example of a *supervised learning* task; we have all the features and all the target classes.\n",
        "\n",
        "Model creation in scikit-learn follows a **data prep -> fit -> predict** process. The \"fit\" function is where the actual model is trained and parameter values are selected, while the \"predict\" function actually takes the trained model and applies it to the new samples.\n",
        "\n",
        "First, we load the nearest neighbor library from scikit-learn:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibJ0k3iYT2Uu"
      },
      "source": [
        "from sklearn import neighbors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elRvY3qnT2U0"
      },
      "source": [
        "Now, we're going to save our feature data into an array called 'X' and our target data into an array called 'y'. We don't *need* to do this, but it is traditional to think of the problem using this notation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R111XUyRT2U2"
      },
      "source": [
        "X = feature_data\n",
        "y = target_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwj0xOkVT2U8"
      },
      "source": [
        "Next, we create our nearest neighbor classifier object:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsSiJavpT2U-"
      },
      "source": [
        "knn = neighbors.KNeighborsClassifier(n_neighbors=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2aq6M17T2VF"
      },
      "source": [
        "And then we *fit* it to the data (i.e., train the classifier)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1y4C1gr2T2VG"
      },
      "source": [
        "knn.fit(X,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQEtTs5NT2VO"
      },
      "source": [
        "Now we have a model! If you're new to this, you've officially built your first machine learning model. If you use \"knn.predict(*[[feature array here]]*)\", you can use your trained model to predict the class of a new iris sample. \n",
        "\n",
        "* What is the predicted class of a new iris sample with feature vector [3,4,5,2]? What is its name? \n",
        "* Do you think this model is overfit or underfit to the iris dataset? Why? \n",
        "* How many neighbors does our model consider when classifying a new sample? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d5lNXh0T2VP"
      },
      "source": [
        "t = knn.predict(np.array([[3,4,5,2]]))[0]\n",
        "print(\"New prediction is class %d, aka. %s\" % (t, target_names[t]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHK3IbwBrU8K"
      },
      "source": [
        "knn.predict(np.array([[3,4,5,2],[1,5,0,-2]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHgHKy7lT2VU"
      },
      "source": [
        "As you may have noted in the previous cell, we've trained this classifier on our *entire dataset*. This typically isn't done in practice and results in overfitting to the data. Here's a bit of a tricky question:\n",
        "\n",
        "* If we use our classifier to predict the classes of the iris samples that were used to train the model itself, what will our overall accuracy be? \n",
        "\n",
        "We can validate our hypothesis fairly easily using either: i) the NumPy technique for calculating accuracy we used earlier in the lab, or ii) scikit-learn's in-house \"accuracy_score()\" function.\n",
        "\n",
        "Let's use our technique first:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOUdvHU8T2VW"
      },
      "source": [
        "accuracy = np.sum(target_data == knn.predict(feature_data)) / target_data.size\n",
        "print (\"Accuracy: \", accuracy * 100, \"%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA3h2XRTT2Vh"
      },
      "source": [
        "and then using scikit-learn's customized function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNL_necHT2Vi"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy = accuracy_score(target_data, knn.predict(feature_data))\n",
        "print (\"Accuracy: \", accuracy * 100, \"%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaW2OKVqT2Vo"
      },
      "source": [
        "We see that our classifier has achieved 100% accuracy (and both calculation methods agree)!\n",
        "\n",
        "**DISCUSSION:** \n",
        "* Why do you think the model was able to achieve such a \"great\" result? \n",
        "* What does this really tell us? \n",
        "* Do you expect the model to perform this well on new data? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_utO_NaT2Vp"
      },
      "source": [
        "### Cross Validation\n",
        "\n",
        "A popular way to mitigate this overfitting issue is to train your model on *some* of the data (the training set) and validate your model on the remaining data (the validation set). You will then select the model/configuration that performs best on the validation data. The train/validate division of the data is usually done with a 70%/30% split. Often, practitioners will use a third data set, the test set (or hold-out set), to get a sense for how their best model performs on unseen, real-world data. In this scenario, you will tune your models to perform best on the validation set and then test their \"real-world\" performance on the unseen test set.\n",
        "\n",
        "Sometimes applications don't have enough data to do these splits meaningfully (e.g., the test data is only a few samples). In these cases, *cross-validation* is a useful technique (and, indeed, has become standard in machine learning practice). \n",
        "\n",
        "The general premise of \"k-folds\" cross validation is to first divide the entire dataset (grey) into a training set (green) and a test set (unseen data, blue). Then, we divide the training set into different folds and use these folds to form new sub-training and sub-test sets. We select the model configuration that performs the best on all of these. The below figure provides a nice visualization for what's going on here:\n",
        "\n",
        "<img src=\"https://scikit-learn.org/stable/_images/grid_search_cross_validation.png\" alt=\"cross-val\" width=\"500\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eZykMe5T2Vr"
      },
      "source": [
        "Accomplishing k-folds cross validation in scikit-learn is a manageable task. First, we divide our data into a train and test set, then we conduct the cross validation and look at the mean scores across the splits, then we conduct our final evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEqOVEXpT2Vs"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(feature_data, target_data, test_size=0.3, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCCQOv38T2Vx"
      },
      "source": [
        "We have divided our data into two sections: training data (70% of the data) and testing data (30% of the data). Now, we will fit our nearest neighbors classifier to the training data using 5-fold cross-validation and see how it performs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yG908nnMnbe"
      },
      "source": [
        "We will be applying `cross_validate` in sklearn to perform cross-validation. We can get both train and validation accuracies using `cross_validate`. Please note that you should set **return_train_score=True** if you want `cross_validate` to return train scores in addition to test scores.\n",
        "\n",
        "You can use the following link to learn more about `cross_validate`:\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGBySD9dMEk6"
      },
      "source": [
        "from sklearn.model_selection import cross_validate\n",
        "scores = cross_validate(knn, X_train, y_train, cv=5,return_train_score=True)\n",
        "\n",
        "print('Mean Train Accuracy:',scores['train_score'].mean()) # returns the mean cross-validation train score\n",
        "print('Mean Validation Accuracy:', scores['test_score'].mean()) # returns the mean cross-validation validation score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3qsGqdFvFIM"
      },
      "source": [
        "scores['train_score']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JOrCSk1vIB0"
      },
      "source": [
        "scores['test_score']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwOdyCDaT2V2"
      },
      "source": [
        "Our cross-validated model has an accuracy of 94% across all the splits on the training data. If we think that is a reasonable value, we can train our final model on the training data and then see how it performs on the held-out test data. \n",
        "\n",
        "##### Comparing classifiers\n",
        "However, to get a true sense for the utility of cross-validation, let's create a second nearest neighbors classifier that uses five neighbors instead of one. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ugHomReT2V3"
      },
      "source": [
        "knn_5 = neighbors.KNeighborsClassifier(n_neighbors=5)\n",
        "scores = cross_validate(knn_5, X_train, y_train, cv=5, return_train_score=True)\n",
        "\n",
        "print('Mean Train Accuracy:',scores['train_score'].mean()) # returns the mean cross-validation train score\n",
        "print('Mean Validation Accuracy:', scores['test_score'].mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7sZCyX0T2V7"
      },
      "source": [
        "\n",
        "Let's train it on the training data and use it to predict the final held-out test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWKkNFFPT2V8"
      },
      "source": [
        "knn_5.fit(X_train, y_train)\n",
        "accuracy = accuracy_score(y_test, knn_5.predict(X_test))\n",
        "print (\"Test set accuracy: \", accuracy * 100, \"%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bG-5YkwtT2V_"
      },
      "source": [
        "And we see our model has a 97.7% accuracy on the held out test data (30% of the original dataset)."
      ]
    }
  ]
}