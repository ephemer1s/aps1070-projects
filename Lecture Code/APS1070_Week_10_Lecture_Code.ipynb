{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "APS1070_Week_10_Lecture_Code.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vk1HIgTbbKt"
      },
      "source": [
        "# APS1070 Week 10 Lecture Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8qP-vDMCsqA"
      },
      "source": [
        "#### Example: Binary classification with mean squared error and Gradient Descent (1-layer ANN)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tmvmSv7bSvm"
      },
      "source": [
        "import math\n",
        "\n",
        "# Data (first column is the bias term)\n",
        "# We have 4 samples and 2 features\n",
        "x = [[1, 0.1,-0.2], \n",
        "     [1,-0.1, 0.9], \n",
        "     [1, 1.2, 0.1], \n",
        "     [1, 1.1, 1.5]]\n",
        "\n",
        "# labels (desired output)\n",
        "# These are binary classes of the 4 samples\n",
        "t = [0, 0, 0, 1]\n",
        "\n",
        "# initial weights\n",
        "w = [1, -1, 1]\n",
        "\n",
        "iterations = 50\n",
        "\n",
        "# As the sample is pacricularly small, an unusual value for the learning rate\n",
        "# is used here\n",
        "learning = 10\n",
        "\n",
        "# This is an explainable code which is not vectorized and is therefore suitable \n",
        "# for demonstration and not suitable for real-world examples\n",
        "def simple_ann(x, w, t, iterations, learning):\n",
        "\n",
        "    E = []\n",
        "    \n",
        "    #iterate over epochs\n",
        "    for ii in range(iterations):\n",
        "        err = [] \n",
        "        y = []\n",
        "        #iterate over all the samples x\n",
        "        for n in range(len(x)):\n",
        "            v = 0\n",
        "            # compute w.x to get the logit\n",
        "            for p in range(len(x[0])):\n",
        "                v = v + x[n][p]*w[p]\n",
        "            \n",
        "            #sigmoidal activation   \n",
        "            y.append(1 / (1 + math.e**(-v))) \n",
        "            \n",
        "            #MSE classification error\n",
        "            err.append((y[n]-t[n])**2)\n",
        "            \n",
        "            #gradient descent to compute new weights\n",
        "            #d is the gradient\n",
        "            for p in range(len(w)):\n",
        "                d = x[n][p]*(y[n]-t[n])*(1-y[n])*(y[n])\n",
        "                w[p] = w[p] - learning*d\n",
        "                \n",
        "        #sum up classification error\n",
        "        E.append(sum(err)/len(x))\n",
        "    \n",
        "    return (y, w, E)\n",
        "\n",
        "(y, w, E) = simple_ann(x, w, t, iterations, learning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TncfaRORb5tN"
      },
      "source": [
        "# The predictions after 50 iterations are quite close to the target [0,0,0,1]\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CV04QGUvMkeA"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.yscale(\"log\")\n",
        "plt.plot(E)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sv7VdSNxcFWT"
      },
      "source": [
        "#### Example: Binary classification with Cross-Entropy and Gradient Descent (1-layer ANN)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytnwNDitduho"
      },
      "source": [
        "import math\n",
        "\n",
        "# data (first column is the bias term)\n",
        "x = [[1, 0.1,-0.2], \n",
        "     [1,-0.1, 0.9], \n",
        "     [1, 1.2, 0.1], \n",
        "     [1, 1.1, 1.5]]\n",
        "\n",
        "# labels (desired output)\n",
        "t = [0, 0, 0, 1]\n",
        "\n",
        "# initial weights\n",
        "w = [1, -1, 1]\n",
        "\n",
        "iterations = 50\n",
        "learning = 10\n",
        "\n",
        "def simple_ann(x, w, t, iterations, learning):\n",
        "\n",
        "    E = []\n",
        "    \n",
        "    #iterate over epochs\n",
        "    for ii in range(iterations):\n",
        "        err = [] \n",
        "        y = []\n",
        "        \n",
        "        #iterate over all the samples x\n",
        "        for n in range(len(x)):\n",
        "            v = 0\n",
        "            \n",
        "            #compute w.x\n",
        "            for p in range(len(x[0])):\n",
        "                v = v + x[n][p]*w[p]\n",
        "            \n",
        "            #sigmoidal activation\n",
        "            y.append(1 / (1 + math.e**(-v))) \n",
        "            \n",
        "            #cross-entropy classification error\n",
        "            err.append(-t[n]*math.log(y[n]) - (1-t[n])*math.log(1-y[n]))\n",
        "\n",
        "            #gradient descent to compute new weights\n",
        "            #gradient is computed according to the new loss function\n",
        "            for p in range(len(w)):\n",
        "                d = x[n][p]*(y[n]-t[n]) #cross_entropy\n",
        "                w[p] = w[p] - learning*d\n",
        "                \n",
        "        #sum up classification error\n",
        "        E.append(sum(err))\n",
        "    \n",
        "    return (y, w, E)\n",
        "\n",
        "(y, w, E) = simple_ann(x, w, t, iterations, learning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnsiF6FDdvM7"
      },
      "source": [
        "# The predictions after 50 iterations are even closer to the target [0,0,0,1]\n",
        "# Cross-entropy loss is a more suitable choice for classification than MSE\n",
        "# The advantage of cross-entropy loss is even more distinctive in real-world cases\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bo0NpnyRMMm0"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.yscale(\"log\")\n",
        "plt.plot(E)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVrJJw84dbiJ"
      },
      "source": [
        "### Exploring the Iris dataset\n",
        "\n",
        "Let us modify the above code to work with the iris data set.\n",
        "\n",
        "To begin, load the iris data into Google Colab. Last time we had some difficulty with this, so it's suggested that you use Chrome or Chromium. Another approach to laod the iris data is shown below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38Qu2LlAIDlQ"
      },
      "source": [
        "# use sklearn.datasets to load iris data\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "features, labels = load_iris(return_X_y=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCcmOIFyIbge"
      },
      "source": [
        "The iris data has 150 samples spread across three classes: \n",
        "1. Iris-setosa, \n",
        "2. Iris-versicolor,\n",
        "3. Iris-virginica. \n",
        "\n",
        "There are three features used: \n",
        "1. sepal length in cm\n",
        "2. sepal width in cm\n",
        "3. petal length in cm\n",
        "4. petal width in cm\n",
        "\n",
        "To keep things simple, let us pick two of the classes and perform binary classification with our sample code. We will select **Iris-setosa** and **Iris-versicolor** to start:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjG7sxA1aA2G"
      },
      "source": [
        "print('Features: \\n ',features)\n",
        "print('labels: \\n', labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sj2vXappwiUA"
      },
      "source": [
        "# classification between setosa (0) and versicolor (1)\n",
        "import numpy as np\n",
        "\n",
        "#selecting only the first 100 samples for binary classification\n",
        "indices = np.array(range(0,100))\n",
        "\n",
        "#setup x matrix\n",
        "x = np.zeros((len(indices), 4 + 1))\n",
        "\n",
        "#columns 1:5 are the features\n",
        "x[:,1:5] = features[indices,:]\n",
        "\n",
        "#column 0 is the bias column\n",
        "x[:,0] = np.ones(len(indices))\n",
        "\n",
        "#labels\n",
        "t = labels[indices]\n",
        "\n",
        "# initial weights\n",
        "w = np.random.rand(5)\n",
        "\n",
        "iterations = 100\n",
        "learning = 0.0001"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vMfTNVFJFif"
      },
      "source": [
        "import math\n",
        "\n",
        "def simple_ann(x, w, t, iterations, learning):\n",
        "\n",
        "    E = []\n",
        "    \n",
        "    #iterate over epochs\n",
        "    for ii in range(iterations):\n",
        "        err = [] \n",
        "        y = []\n",
        "        \n",
        "        #iterate over all the samples x\n",
        "        for n in range(len(x)):\n",
        "            \n",
        "            v = 0\n",
        "            #compute w.x\n",
        "            for p in range(len(x[0])):\n",
        "                v = v + x[n,p]*w[p]\n",
        "            \n",
        "            #sigmoidal activation\n",
        "            y.append(1 / (1 + math.e**(-v))) \n",
        "            \n",
        "\n",
        "            #cross-entropy classification error\n",
        "            err.append(-t[n]*math.log(y[n]+ 0.000001) - (1-t[n])*math.log(1-y[n]+ 0.000001))\n",
        "            \n",
        "            #logistic-cross-entropy classification error\n",
        "            #err.append(t[n]*math.log(1 + math.e**(-v)) + (1-t[n])*math.log(1 + math.e**(v)))            \n",
        "\n",
        "            #gradient descent to compute new weights\n",
        "            for p in range(len(w)):\n",
        "                d = x[n][p]*(y[n]-t[n]) #cross_entropy\n",
        "                w[p] = w[p] - learning*d\n",
        "                \n",
        "        #sum up classification error\n",
        "        E.append(sum(err))\n",
        "    \n",
        "    return (y, w, E)\n",
        "\n",
        "(y, w, E) = simple_ann(x, w, t, iterations, learning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgPIaMRLzjXC"
      },
      "source": [
        "# We consider a treshold of 0.5 for interpretting the predictions\n",
        "misclas_error=np.mean((np.round(y)-t)**2)\n",
        "print(\"Misclassification error: \",misclas_error)\n",
        "\n",
        "# In most cases, the predictions are considerably away from 0.5\n",
        "for i in range(len(t)):\n",
        "  print(\"Target: \",t[i],\"Prediction\", y[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZO8CMusz29hQ"
      },
      "source": [
        "We're able to classify iris-setosa and iris-versicolor with reasonably low misclassification error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKtAXhNZJUok"
      },
      "source": [
        "# classification versicolor (1) and virginica (2)\n",
        "import numpy as np\n",
        "\n",
        "#selecting samples associated with classes 1,2\n",
        "indices = np.array(range(50,150))\n",
        "\n",
        "#setup x matrix\n",
        "x = np.zeros((len(indices), 4 + 1))\n",
        "x[:,1:5] = features[indices,:]\n",
        "\n",
        "#add bias column\n",
        "x[:,0] = np.ones(len(indices))\n",
        "\n",
        "#subtracting 1 from labels to get a binary target\n",
        "t = labels[indices]-1\n",
        "\n",
        "# initial weights\n",
        "w = np.random.rand(5)\n",
        "\n",
        "iterations = 100\n",
        "learning = 0.0001\n",
        "\n",
        "\n",
        "import math\n",
        "\n",
        "def simple_ann(x, w, t, iterations, learning):\n",
        "\n",
        "    E = []\n",
        "    \n",
        "    #iterate over epochs\n",
        "    for ii in range(iterations):\n",
        "        err = [] \n",
        "        y = []\n",
        "        \n",
        "        #iterate over all the samples x\n",
        "        for n in range(len(x)):\n",
        "            \n",
        "            v = 0\n",
        "            #compute w.x\n",
        "            for p in range(len(x[0])):\n",
        "                v = v + x[n,p]*w[p]\n",
        "            \n",
        "            #sigmoidal activation\n",
        "            y.append(1 / (1 + math.e**(-v))) \n",
        "            \n",
        "\n",
        "            #cross-entropy classification error\n",
        "            err.append(-t[n]*math.log(y[n]+ 0.000001) - (1-t[n])*math.log(1-y[n]+ 0.000001))\n",
        "\n",
        "            #gradient descent to compute new weights\n",
        "            for p in range(len(w)):\n",
        "                d = x[n][p]*(y[n]-t[n]) #cross_entropy\n",
        "                w[p] = w[p] - learning*d\n",
        "                \n",
        "        #sum up classification error\n",
        "        E.append(sum(err))\n",
        "    \n",
        "    return (y, w, E)\n",
        "\n",
        "(y, w, E) = simple_ann(x, w, t, iterations, learning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5DlhozN2gOW"
      },
      "source": [
        "# We consider a treshold of 0.5 for interpretting the predictions\n",
        "misclas_error=np.mean((np.round(y)-t)**2)\n",
        "print(\"Misclassification error: \",misclas_error)\n",
        "\n",
        "# In many cases, the prediction is very close to 0.5\n",
        "for i in range(len(t)):\n",
        "  print(\"Target: \",t[i],\"Prediction\", y[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iwvTlW43L8h"
      },
      "source": [
        "The performance on the iris-versicolor and iris virginica is not as good. To find out why, we will try to visualize the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wb_g2BEdU_Fu"
      },
      "source": [
        "### Visualize Iris Dataset\n",
        "Since the Iris dataset has only 4 inputs we can try to visualize it on a 2-dimensional plane to get a better idea of what is happening."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6SleB-DVS25"
      },
      "source": [
        "#scatter plot of iris-setosa and iris-versicolor\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "indices = np.array(range(0,100))\n",
        "\n",
        "selected_features = features[indices,:]\n",
        "selected_labels = labels[indices]\n",
        "\n",
        "feature_name = ['sepal length in cm', 'sepal width in cm', 'petal length in cm', 'petal width in cm']\n",
        "\n",
        "x_index = 0\n",
        "y_index = 1\n",
        "\n",
        "plt.figure(figsize=(5, 4))\n",
        "plt.scatter(selected_features[:,x_index], selected_features[:,y_index], c= selected_labels)\n",
        "plt.colorbar(ticks=[0, 1, 2])\n",
        "plt.xlabel(feature_name[x_index])\n",
        "plt.ylabel(feature_name[y_index])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcLbUFswvBCE"
      },
      "source": [
        "# scatter plot of iris-versicolor and iris-virginica\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "indices = np.array(range(50,150))\n",
        "\n",
        "selected_features = features[indices,:]\n",
        "selected_labels = labels[indices]\n",
        "\n",
        "feature_name = ['sepal length in cm', 'sepal width in cm', 'petal length in cm', 'petal width in cm']\n",
        "\n",
        "x_index = 0\n",
        "y_index = 1\n",
        "\n",
        "plt.figure(figsize=(5, 4))\n",
        "plt.scatter(selected_features[:,x_index], selected_features[:,y_index], c= selected_labels)\n",
        "plt.colorbar(ticks=[0, 1, 2])\n",
        "plt.xlabel(feature_name[x_index])\n",
        "plt.ylabel(feature_name[y_index])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Cqsdyg1de8M"
      },
      "source": [
        "### Nonlinear Separation\n",
        "Our 1-layer ANN was only successful at one of the binary classification combinations. A 1-layer ANN is unable to handle nonlinear separations (or decisions boundaries). To address this we can introduce a second layer known as a hidden layer. How could we do this?\n",
        "\n",
        "We can just include an additional 1-layer networks as shown in the image below. \n",
        "\n",
        "![alt text](https://miro.medium.com/max/1000/1*sX6T0Y4aa3ARh7IBS_sdqw.png)\n",
        "\n",
        "\n",
        "We would follow the same process as with the 1-layer network:\n",
        "\n",
        "1. write out the equations for the forward pass\n",
        "2. error term can stay the same MSE or Cross-Entropy\n",
        "3. Gradient descent would be applied now to two layers of weights\n",
        "\n",
        "First we would consider the forward pass for a 2-layer ANN. We could use a sigmoidal (logistic) activation function to keep things consistent with our earlier example. Note that the activation function will be applied once on the hidden layer, and also on the output layer.\n",
        "\n",
        "Computing the gradient with respect to the different layers of weights will become more difficult, but still manageable. There are just some additional terms in the chain rule. The second layer weights will be almost identical to what we computed for a 1-layer network, except the input will be the hidden layer activation.\n",
        "\n",
        "Instead of spending the time to compute gradients with each change to the network, which can be a fun mathematical exercise, we will instead focus on using the PyTorch libraries which handle all of this internally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5l1uHHvTRGiI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}