{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "APS1070 - Week 11 Lecture Code.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MD1j2BeWPkgt"
      },
      "source": [
        "# APS1070 Week 11 Lecture Code\n",
        "## Part 1a - 2-layer neural network\n",
        "\n",
        "In this example we will implement a 2-layer neural network from scratch. This network is using a squared error loss for a multiclass classification problem, ideally we should be using a softmax loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "P4xYawtrSkNX",
        "outputId": "0b9a22cd-533f-4008-b9e4-00fd0926c991"
      },
      "source": [
        "# load \"Iris_3class.csv\" to Google Colab\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-67b52198-fb61-44d2-9075-0917720a0475\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-67b52198-fb61-44d2-9075-0917720a0475\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Iris_3class.csv to Iris_3class.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2l_TNr3n72A",
        "outputId": "0d77659b-2b3f-46b2-b20c-8d21d687b327"
      },
      "source": [
        "import pandas as pd\n",
        "raw_data = pd.read_csv(\"Iris_3class.csv\", header = None)\n",
        "raw_data.values.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(150, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EG47Tnmn72B",
        "outputId": "1e201c8c-5630-4313-ddd6-5c392118e1dd"
      },
      "source": [
        "import numpy as np\n",
        "raw_data = raw_data.values\n",
        "\n",
        "X_train = raw_data[:,:4]\n",
        "y_train = raw_data[:,4:5].astype(int)\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_train.dtype, y_train.dtype)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(150, 4) (150, 1)\n",
            "float64 int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDd17mrCTJeP"
      },
      "source": [
        "convert labels (ground truths) into one-hot vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcYgv_I5R_LE"
      },
      "source": [
        "#Convert array to one-hot encoding\n",
        "def to_one_hot(Y):\n",
        "    n_col = np.amax(Y) + 1\n",
        "    binarized = np.zeros((len(Y), n_col))\n",
        "    for i in range(len(Y)):\n",
        "        binarized[i, Y[i]] = 1.\n",
        "    return binarized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckspIetcSEou",
        "outputId": "10cb2c10-7c69-4f14-f4aa-1a24e7e27ca3"
      },
      "source": [
        "y_train = to_one_hot(y_train)\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_train.dtype, y_train.dtype)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(150, 4) (150, 3)\n",
            "float64 float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNbpBYIrTBLu",
        "outputId": "49348d5e-45d0-417b-dad9-c258609c7a85"
      },
      "source": [
        "#verify one-hot encoding\n",
        "y_train[0:5,:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c90UmCzGJPLd"
      },
      "source": [
        "At its core a 2-layer neural network is just a few lines of code. \n",
        "\n",
        "Note that we have excluded the bias terms to keep things simple."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6C6x8K6jJDMt"
      },
      "source": [
        "#define sigmoid\n",
        "def sigmoid(x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "\n",
        "def ann(W, X_train, y_train):\n",
        "\n",
        "  #Weights\n",
        "  w0 = W[:20].reshape(4,5)\n",
        "  w1 = W[20:].reshape(5,3)\n",
        "\n",
        "  #Feed forward\n",
        "  layer0 = X_train\n",
        "  layer1 = sigmoid(np.dot(layer0, w0))\n",
        "  layer2 = sigmoid(np.dot(layer1, w1)) #predictions\n",
        "\n",
        "  return layer2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPLLL-xzJfsw"
      },
      "source": [
        "Most of the complexity is introduced in order to train the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qs5GCmM5niXV"
      },
      "source": [
        "#define sigmoid\n",
        "def sigmoid(x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "\n",
        "def ann(W, X_train, y_train):\n",
        "\n",
        "  #Weights\n",
        "  w0 = W[:20].reshape(4,5)\n",
        "  w1 = W[20:].reshape(5,3)\n",
        "\n",
        "  #Feed forward\n",
        "  layer0 = X_train\n",
        "  layer1 = sigmoid(np.dot(layer0, w0))\n",
        "  layer2 = sigmoid(np.dot(layer1, w1)) #predictions\n",
        " \n",
        "  #Back propagation using gradient descent\n",
        "  dw0, dw1 = np.zeros((4,5)), np.zeros((5,3))\n",
        "\n",
        "  #calculate partial derivatives\n",
        "  dL_du_hat = layer2-y_train\n",
        "  du_hat_du = layer2*(1-layer2)\n",
        "  du_dv_hat = w1.T\n",
        "  dv_hat_dv = layer1*(1-layer1)\n",
        "  dv_dw0 = X_train\n",
        "  du_dw1 = layer1\n",
        "\n",
        "  #gradients\n",
        "  dw1 += du_dw1.T.dot(dL_du_hat*du_hat_du)\n",
        "  dw0 += dv_dw0.T.dot((dL_du_hat*du_hat_du).dot(du_dv_hat)*(dv_hat_dv))\n",
        "  \n",
        "  #combine gradients\n",
        "  dW = np.array(list(dw0.flatten()) + list(dw1.flatten()))\n",
        "\n",
        "  #squared error\n",
        "  error = 0.5*np.sum((layer2 - y_train)**2)\n",
        "\n",
        "  return (error, dW, layer2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmxW9MtoRBil"
      },
      "source": [
        "Before training the network, let us verify the gradients were calculated correctly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUd0wkNnL7BJ",
        "outputId": "e70d09e7-cc2d-4fa1-ffc2-21dd355e9dce"
      },
      "source": [
        "#initialize weights\n",
        "w0 = 2*np.random.random((4, 5)) - 1\n",
        "w1 = 2*np.random.random((5, 3)) - 1\n",
        "\n",
        "#combine weights\n",
        "W = np.array(list(w0.flatten()) + list(w1.flatten()))\n",
        "\n",
        "#compute gradients analytically\n",
        "(error, dW, y_hat) = ann(W, X_train, y_train)\n",
        "\n",
        "#compute gradients numerically\n",
        "dW_num = np.zeros((len(W),1))\n",
        "\n",
        "for ind in range(len(W)):\n",
        "  #reset gradients\n",
        "  We1 = np.array(list(w0.flatten()) + list(w1.flatten()))\n",
        "  We2 = np.array(list(w0.flatten()) + list(w1.flatten()))\n",
        "  \n",
        "  #increment slightly\n",
        "  We1[ind] = We1[ind] + 0.000001\n",
        "  We2[ind] = We2[ind] - 0.000001\n",
        "  \n",
        "  #compute errors\n",
        "  (error_e1, dW_e1, y_hat) = ann(We1, X_train, y_train)\n",
        "  (error_e2, dW_e2, y_hat) = ann(We2, X_train, y_train)\n",
        "  \n",
        "  #calculate each gradient\n",
        "  grad_num = (error_e1-error_e2)/0.000002\n",
        "  \n",
        "  #display difference\n",
        "  print(round(abs(grad_num-dW[ind]),4), grad_num, dW[ind])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0 -0.39696112708043074 -0.3969611315255835\n",
            "0.0 9.403257656970254 9.40325765691772\n",
            "0.0 12.15712492808052 12.157124930724983\n",
            "0.0 0.6168242379089861 0.6168242404558764\n",
            "0.0 -4.446974493532707 -4.446974497290471\n",
            "0.0 -0.14841754847338962 -0.148417551415719\n",
            "0.0 6.121654791968467 6.121654791233746\n",
            "0.0 8.225682563534065 8.225682552333968\n",
            "0.0 0.26866472779829564 0.26866472405056696\n",
            "0.0 -3.1055338425289847 -3.105533845770081\n",
            "0.0 -0.39701716758600014 -0.39701717160108363\n",
            "0.0 3.2271908096959123 3.2271908104776563\n",
            "0.0 3.6861417029854238 3.6861417067057656\n",
            "0.0 0.5398442866066944 0.5398442873110401\n",
            "0.0 -1.0426430065990644 -1.0426430110362779\n",
            "0.0 -0.14719170593480158 -0.1471917110667333\n",
            "0.0 0.6313751086395314 0.6313751035999894\n",
            "0.0 0.650397424806215 0.6503974211739173\n",
            "0.0 0.19697923647754578 0.19697924224597488\n",
            "0.0 -0.08685636032623734 -0.08685635914200414\n",
            "0.0 0.10910099490502034 0.10910098363690418\n",
            "0.0 0.12217915923429246 0.12217915657541749\n",
            "0.0 0.15286622101484681 0.15286622511636389\n",
            "0.0 10.743150042458183 10.743150047211545\n",
            "0.0 7.959684445779658 7.959684445529014\n",
            "0.0 7.911220606615643 7.911220610936008\n",
            "0.0 9.793415188141807 9.793415194361181\n",
            "0.0 10.374812809743617 10.374812805973814\n",
            "0.0 10.833335387872012 10.833335389355055\n",
            "0.0 8.995785066190365 8.995785075239851\n",
            "0.0 11.699050958213775 11.69905096156375\n",
            "0.0 12.323495262478446 12.3234952617399\n",
            "0.0 -0.0830141999585976 -0.0830142011664494\n",
            "0.0 0.899803445975067 0.8998034537190922\n",
            "0.0 1.1951320857406245 1.19513208657262\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6W5pxdJPn9k"
      },
      "source": [
        "Train neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SZkGHblpF0J"
      },
      "source": [
        "#initialize weights\n",
        "w0 = 2*np.random.random((4, 5)) - 1\n",
        "w1 = 2*np.random.random((5, 3)) - 1\n",
        "\n",
        "#combine weights into a single vector\n",
        "W = np.array(list(w0.flatten()) + list(w1.flatten()))\n",
        "\n",
        "#train network\n",
        "n = 0.001 #learning rate\n",
        "errors = []\n",
        "for i in range(100000):\n",
        "  (error, dW, y_hat) = ann(W, X_train, y_train)\n",
        "  W += -dW * n\n",
        "  errors.append(error)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0enFo7Z-UDUJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba18cb9c-2185-4156-d079-f3fbb61d55c0"
      },
      "source": [
        "#examine predictions on training data\n",
        "np.round(y_hat,1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1. , 0. , 0. ],\n",
              "       [1. , 0. , 0. ],\n",
              "       [1. , 0. , 0. ],\n",
              "       [1. , 0. , 0. ],\n",
              "       [1. , 0. , 0. ],\n",
              "       [1. , 0. , 0. ],\n",
              "       [1. , 0. , 0. ],\n",
              "       [1. , 0. , 0. ],\n",
              "       [1. , 0. , 0. ],\n",
              "       [1. , 0. , 0. ],\n",
              "       [1. , 0. , 0. ],\n",
              "       [1. , 0. , 0. ],\n",
              "       [1. , 0. , 0. ],\n",
              "       [1. , 0. , 0. ],\n",
              "       [1. , 0. , 0. ],\n",
              "       [1. , 0. , 0. ],\n",
              "       [1. , 0. , 0. ],\n",
              "       [1. , 0. , 0. ],\n",
              "       [1. , 0. , 0. ],\n",
              "       [1. , 0. , 0. ],\n",
              "       [1. , 0. , 0. ],\n",
              "       [1. , 0. , 0. ],\n",
              "       [1. , 0. , 0. ],\n",
              "       [1. , 0. , 0. ],\n",
              "       [1. , 0. , 0. ],\n",
              "       [1. , 0. , 0. ],\n",
              "       [1. , 0. , 0. ],\n",
              "       [1. , 0. , 0. ],\n",
              "       [1. , 0. , 0. ],\n",
              "       [1. , 0. , 0. ],\n",
              "       [1. , 0. , 0. ],\n",
              "       [1. , 0. , 0. ],\n",
              "       [1. , 0. , 0. ],\n",
              "       [1. , 0. , 0. ],\n",
              "       [1. , 0. , 0. ],\n",
              "       [1. , 0. , 0. ],\n",
              "       [1. , 0. , 0. ],\n",
              "       [1. , 0. , 0. ],\n",
              "       [1. , 0. , 0. ],\n",
              "       [1. , 0. , 0. ],\n",
              "       [1. , 0. , 0. ],\n",
              "       [1. , 0. , 0. ],\n",
              "       [1. , 0. , 0. ],\n",
              "       [1. , 0. , 0. ],\n",
              "       [1. , 0. , 0. ],\n",
              "       [1. , 0. , 0. ],\n",
              "       [1. , 0. , 0. ],\n",
              "       [1. , 0. , 0. ],\n",
              "       [1. , 0. , 0. ],\n",
              "       [1. , 0. , 0. ],\n",
              "       [0. , 1. , 0. ],\n",
              "       [0. , 1. , 0. ],\n",
              "       [0. , 1. , 0. ],\n",
              "       [0. , 1. , 0. ],\n",
              "       [0. , 1. , 0. ],\n",
              "       [0. , 1. , 0. ],\n",
              "       [0. , 1. , 0. ],\n",
              "       [0. , 1. , 0. ],\n",
              "       [0. , 1. , 0. ],\n",
              "       [0. , 1. , 0. ],\n",
              "       [0. , 1. , 0. ],\n",
              "       [0. , 1. , 0. ],\n",
              "       [0. , 1. , 0. ],\n",
              "       [0. , 1. , 0. ],\n",
              "       [0. , 1. , 0. ],\n",
              "       [0. , 1. , 0. ],\n",
              "       [0. , 0.9, 0. ],\n",
              "       [0. , 1. , 0. ],\n",
              "       [0. , 0.7, 0.2],\n",
              "       [0. , 1. , 0. ],\n",
              "       [0. , 0.4, 0.6],\n",
              "       [0. , 1. , 0. ],\n",
              "       [0. , 0.3, 0.7],\n",
              "       [0. , 1. , 0. ],\n",
              "       [0. , 1. , 0. ],\n",
              "       [0. , 1. , 0. ],\n",
              "       [0. , 1. , 0. ],\n",
              "       [0. , 1. , 0. ],\n",
              "       [0. , 1. , 0. ],\n",
              "       [0. , 1. , 0. ],\n",
              "       [0. , 1. , 0. ],\n",
              "       [0. , 1. , 0. ],\n",
              "       [0. , 1. , 0. ],\n",
              "       [0. , 0. , 1. ],\n",
              "       [0. , 0.7, 0.3],\n",
              "       [0. , 1. , 0. ],\n",
              "       [0. , 1. , 0. ],\n",
              "       [0. , 1. , 0. ],\n",
              "       [0. , 1. , 0. ],\n",
              "       [0. , 1. , 0. ],\n",
              "       [0. , 1. , 0. ],\n",
              "       [0. , 1. , 0. ],\n",
              "       [0. , 1. , 0. ],\n",
              "       [0. , 1. , 0. ],\n",
              "       [0. , 1. , 0. ],\n",
              "       [0. , 1. , 0. ],\n",
              "       [0. , 1. , 0. ],\n",
              "       [0. , 1. , 0. ],\n",
              "       [0. , 1. , 0. ],\n",
              "       [0. , 1. , 0. ],\n",
              "       [0. , 0. , 1. ],\n",
              "       [0. , 0. , 1. ],\n",
              "       [0. , 0. , 1. ],\n",
              "       [0. , 0. , 1. ],\n",
              "       [0. , 0. , 1. ],\n",
              "       [0. , 0. , 1. ],\n",
              "       [0. , 0. , 1. ],\n",
              "       [0. , 0. , 1. ],\n",
              "       [0. , 0. , 1. ],\n",
              "       [0. , 0. , 1. ],\n",
              "       [0. , 0.3, 0.7],\n",
              "       [0. , 0. , 1. ],\n",
              "       [0. , 0. , 1. ],\n",
              "       [0. , 0. , 1. ],\n",
              "       [0. , 0. , 1. ],\n",
              "       [0. , 0. , 1. ],\n",
              "       [0. , 0. , 1. ],\n",
              "       [0. , 0. , 1. ],\n",
              "       [0. , 0. , 1. ],\n",
              "       [0. , 0. , 1. ],\n",
              "       [0. , 0. , 1. ],\n",
              "       [0. , 0. , 1. ],\n",
              "       [0. , 0. , 1. ],\n",
              "       [0. , 0.1, 0.9],\n",
              "       [0. , 0. , 1. ],\n",
              "       [0. , 0. , 1. ],\n",
              "       [0. , 0.2, 0.8],\n",
              "       [0. , 0.2, 0.8],\n",
              "       [0. , 0. , 1. ],\n",
              "       [0. , 0.2, 0.8],\n",
              "       [0. , 0. , 1. ],\n",
              "       [0. , 0.5, 0.5],\n",
              "       [0. , 0. , 1. ],\n",
              "       [0. , 0.3, 0.7],\n",
              "       [0. , 0. , 1. ],\n",
              "       [0. , 0. , 1. ],\n",
              "       [0. , 0. , 1. ],\n",
              "       [0. , 0. , 1. ],\n",
              "       [0. , 0.2, 0.8],\n",
              "       [0. , 0.1, 0.9],\n",
              "       [0. , 0. , 1. ],\n",
              "       [0. , 0.2, 0.8],\n",
              "       [0. , 0. , 1. ],\n",
              "       [0. , 0. , 1. ],\n",
              "       [0. , 0. , 1. ],\n",
              "       [0. , 0. , 1. ],\n",
              "       [0. , 0. , 1. ],\n",
              "       [0. , 0. , 1. ],\n",
              "       [0. , 0. , 1. ],\n",
              "       [0. , 0. , 1. ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eyv7qoFcVvPg"
      },
      "source": [
        "The above is just a demonstration of building a neural network from scratch. To properly assess this network you should create a validation data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evqmxxgjXOUd"
      },
      "source": [
        "## Part 1b - PyTorch\n",
        "\n",
        "The following is an example of a 1-layer and 2-layer neural network using PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdykz0ZxXbR2"
      },
      "source": [
        "PyTorch - 1-layer neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMiAfLmxW1p1"
      },
      "source": [
        "import torch\n",
        "\n",
        "x = torch.ones(5)  # input tensor\n",
        "y = torch.zeros(3)  # expected output\n",
        "\n",
        "w = torch.randn(5, 3, requires_grad=True)\n",
        "b = torch.randn(3, requires_grad=True)\n",
        "z = torch.matmul(x, w)+b\n",
        "\n",
        "# assumes binary output\n",
        "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "227UvhofXll1"
      },
      "source": [
        "obtain gradients for 1-layer neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ntotk5fW5LA",
        "outputId": "42346683-ae47-46e6-90a0-6d6e9fc62877"
      },
      "source": [
        "loss.backward()\n",
        "print(w.grad)\n",
        "print(b.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.2804, 0.3105, 0.1160],\n",
            "        [0.2804, 0.3105, 0.1160],\n",
            "        [0.2804, 0.3105, 0.1160],\n",
            "        [0.2804, 0.3105, 0.1160],\n",
            "        [0.2804, 0.3105, 0.1160]])\n",
            "tensor([0.2804, 0.3105, 0.1160])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOtMEHf_Xj3w"
      },
      "source": [
        "PyTorch - 2-layer neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uTVjwLkzMXr"
      },
      "source": [
        "#2-layer neural network\n",
        "import torch\n",
        "\n",
        "num_hidden = 3\n",
        "x = torch.ones(5)  # input tensor\n",
        "y = torch.zeros(3)  # expected output\n",
        "\n",
        "# layer 1\n",
        "w = torch.randn(5, num_hidden, requires_grad=True)\n",
        "b = torch.randn(num_hidden, requires_grad=True)\n",
        "z = torch.matmul(x, w)+b\n",
        "z = torch.sigmoid(z)\n",
        "\n",
        "# layer 2\n",
        "w2 = torch.randn(num_hidden, 3, requires_grad=True)\n",
        "b2 = torch.randn(3, requires_grad=True)\n",
        "z2 = torch.matmul(z, w2)+b2\n",
        "\n",
        "# assumes binary output\n",
        "loss = torch.nn.functional.binary_cross_entropy_with_logits(z2, y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpej6hLZYGJ_"
      },
      "source": [
        "obtain gradients for 2-layer neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9WT-0L_0AZm",
        "outputId": "953e2a15-1e96-4872-e4dd-983ef359a38b"
      },
      "source": [
        "loss.backward()\n",
        "print(w.grad)\n",
        "print(b.grad)\n",
        "print(w2.grad)\n",
        "print(b2.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.1788, 0.0107, 0.0060],\n",
            "        [0.1788, 0.0107, 0.0060],\n",
            "        [0.1788, 0.0107, 0.0060],\n",
            "        [0.1788, 0.0107, 0.0060],\n",
            "        [0.1788, 0.0107, 0.0060]])\n",
            "tensor([0.1788, 0.0107, 0.0060])\n",
            "tensor([[0.2061, 0.2360, 0.0221],\n",
            "        [0.1278, 0.1463, 0.0137],\n",
            "        [0.0088, 0.0101, 0.0009]])\n",
            "tensor([0.2729, 0.3125, 0.0293])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_3rNNw5YQzq"
      },
      "source": [
        "PyTorch computational graphs make gradient calculations stright forward. Much of this is hidden away allowing you to focus more on developing and testing your model architectures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jyyPknOZSUf"
      },
      "source": [
        "## Part 1c - PyTorch Full Example\n",
        "\n",
        "see file \"PyTorch_Example_MNIST\" for a full example using PyTorch."
      ]
    }
  ]
}