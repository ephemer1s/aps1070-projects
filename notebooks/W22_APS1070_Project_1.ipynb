{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "W22_APS1070_Project_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08owH-AVyZCX"
      },
      "source": [
        "# APS1070\n",
        "#### Project 1 --- Basic Principles and Models \n",
        "**Deadline: Feb 4, 11PM - 10 percent**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRZ3wBoe398S"
      },
      "source": [
        "**Academic Integrity**\n",
        "\n",
        "This project is individual - it is to be completed on your own. If you have questions, please post your query in the APS1070 Piazza Q&A forums (the answer might be useful to others!).\n",
        "\n",
        "Do not share your code with others, or post your work online. Do not submit code that you have not written yourself. Students suspected of plagiarism on a project, midterm or exam will be referred to the department for formal discipline for breaches of the Student Code of Conduct."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qonbueFGyemb"
      },
      "source": [
        "Name: ___  *(here and elsewhere, please replace the underscore with your answer)*\n",
        "\n",
        "Student ID: ___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dykrw3cyy7PF"
      },
      "source": [
        "##**Marking Scheme:**\n",
        "\n",
        "This project is worth **10 percent** of your final grade.\n",
        "\n",
        "Draw a plot or table where necessary to summarize your findings. \n",
        "\n",
        "**Practice Vectorized coding**: If you need to write a loop in your solution, think about how you can implement the same functionality with vectorized operations. Try to avoid loops as much as possible (in some cases, loops are inevitable).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How to submit **(HTML + IPYNB)**\n",
        "\n",
        "1. Download your notebook: `File -> Download .ipynb`\n",
        "\n",
        "2. Click on the Files icon on the far left menu of Colab\n",
        "\n",
        "3. Select & upload your `.ipynb` file you just downloaded, and then obtain its path (right click) (you might need to hit the Refresh button before your file shows up)\n",
        "\n",
        "\n",
        "4. execute the following in a Colab cell:\n",
        "```\n",
        "%%shell\n",
        "jupyter nbconvert --to html /PATH/TO/YOUR/NOTEBOOKFILE.ipynb\n",
        "```\n",
        "\n",
        "5. An HTML version of your notebook will appear in the files, so you can download it.\n",
        "\n",
        "6. Submit **both** <font color='red'>`HTML` and `IPYNB`</font>  files on Quercus for grading.\n",
        "\n",
        "\n",
        "\n",
        "Ref: https://stackoverflow.com/a/64487858 \n",
        "\n"
      ],
      "metadata": {
        "id": "dBW_phAt5wYk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYlo7mIXyZFe"
      },
      "source": [
        "# Project 1 [10 Marks] \n",
        "Let's apply the tools we have learned in the tutorial to a new dataset.\n",
        "\n",
        "We're going to work with a breast cancer dataset. Download it using the cell below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFo8KVcryZFe"
      },
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "dataset = load_breast_cancer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOGmwyQNyZFh"
      },
      "source": [
        "## Part 1: Getting started [2 Marks]\n",
        "First off, take a look at the `data`, `target` and `feature_names` entries in the `dataset` dictionary. They contain the information we'll be working with here. Then, create a Pandas DataFrame called `df` containing the data and the targets, with the feature names as column headings. If you need help, see [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) for more details on how to achieve this. **[0.4]**\n",
        "* How many features do we have in this dataset? ___\n",
        "* How many observations have a 'mean area' of greater than 700? ___\n",
        "* How many participants tested `Malignant`? ___\n",
        "* How many participants tested `Benign`? ___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyqBEQXGq-B0"
      },
      "source": [
        "### Splitting the data\n",
        "It is best practice to have a training set (from which there is a rotating validation subset) and a test set. Our aim here is to (eventually) obtain the best accuracy we can on the test set (we'll do all our tuning on the training/validation sets, however.) \n",
        "\n",
        "**Split the dataset** into a train and a test set **\"70:30\"**, use **``random_state=0``**. The test set is set aside (untouched) for final evaluation, once hyperparameter optimization is complete. **[0.5]**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5equ6ied8eg"
      },
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_jNtkfce9Eg"
      },
      "source": [
        "### Effect of Standardization (Visual)\n",
        "Use `seaborn.lmplot` ([help here](https://seaborn.pydata.org/generated/seaborn.lmplot.html)) to visualize a few features of the training set. Draw a plot where the x-axis is ``worst smoothness``, the y-axis is ``worst fractal dimension,`` and the color of each datapoint indicates its class.  **[0.5]**\n",
        "\n",
        "Standardizing the data is often critical in machine learning. Show a plot as above, but with two features with very different scales. Standardize the data and plot those features again. What's different? Based on your observation, what is the advantage of standardization? **[0.6]**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J684emSV8mJd"
      },
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBmo0-W1yZFs"
      },
      "source": [
        "## Part 2: KNN Classifier without Standardization [2 Marks]\n",
        "Normally, standardizing data is a key step in preparing data for a KNN classifier. However, for educational purposes, let's first try to build a model without standardization. Let's create a KNN classifier to predict whether a patient has a malignant or benign tumor. \n",
        "\n",
        "Follow these steps: \n",
        "\n",
        "1.   Train a KNN Classifier using cross-validation on the dataset. Sweep `k` (number of neighbours) from 1 to 100, and show a plot of the mean cross-validation accuracy vs `k`. **[1]**\n",
        "2.   What is the best `k`? What is the highest cross-validation accuracy? **[0.5]**\n",
        "3. Comment on  which ranges of `k` lead to underfitted or overfitted models (hint: compare training and validation curves!). **[0.5]**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyEV-mVSAMaV"
      },
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCyFm5S3srCO"
      },
      "source": [
        "## Part 3: Feature Selection [3 Marks]\n",
        "In this part, we aim to investigate the importance of each feature on the final classification accuracy. \n",
        "If we want to try every possible combination of features, we would have to test  $2^F$ different cases,  where F is the number of features, and in each case, we have to do a hyperparameter search (finding K, in KNN using cross-validation). That will take days!. \n",
        "\n",
        "To find more important features we will use a decision tree. based on a decision tree we can compute feature importance that is a metric for our feature selection (code is provided below).\n",
        "\n",
        "You can use [this link](https://machinelearningmastery.com/calculate-feature-importance-with-python/\n",
        ") to get familiar with extracting the feature impotance order of machine learning algorithms in Python.\n",
        "\n",
        "After we identified and removed the least important feature and evaluated a new KNN model on the new set of features, if the stop conditions (see step 7 below) are not met, we need to repeat the process and remove another feature.\n",
        "\n",
        "\n",
        "Design a function ( `Feature_selector`) that accepts your dataset (X_train , y_train) and a threshold as inputs and: **[1]**\n",
        "1. Fits a decision tree classifier on the training set.\n",
        "\n",
        "2. Extracts the feature importance order of the decision tree model.\n",
        "\n",
        "3. Removes the least important feature based on step 2. \n",
        "4. Then, a KNN model is trained on the remaining features. The number of neighbors (`k`) for each KNN model should be tuned using a 5-fold cross-validation.\n",
        "5. Store the best `mean cross-validation` score and the corresponding `k` (number of neighbours) value in two lists.\n",
        "6. Go back to step 3 and follow all the steps until you meet the stop condition (step 7). \n",
        " \n",
        "7. We will stop this process when (1) there is only one feature left, or (2) our cross-validation accuracy is dropped significantly compared to a model that uses all the features. In this function, we accept a threshold as an input argument. For example, if threshold=0.95 we do not continue removing features if our mean cross-validation accuracy after tuning `k` is bellow **0.95 $\\times$ Full Feature cross-validation accuracy**.\n",
        "\n",
        "8. Your function returns the list of removed features, and the corresponding mean cross-validation accuracy and `k` value when a feature was removed.\n",
        "\n",
        "* Visualize your results by plotting the mean cross-validation accuracy (with a tuned `k` on y axis) vs. the on the number of features (x axis). This plot describes: what is the best cv score with 1 feature, 2 features, 3 features ... and all the features. **[0.5]**\n",
        "\n",
        "* Plot the best value of `k` (y-axis) vs. the number of features. This plot explains the trend of number of neighbours with respect to the number of features.  **[0.5]**\n",
        "\n",
        "* State what is the number of essential features for classification and justify your answer. **[1]**\n",
        "  \n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPcG6_UIdAaT"
      },
      "source": [
        "You can use the following piece of code to start training a decision tree classifier and obtain its feature importance order. \n",
        "```\n",
        "from sklearn import tree\n",
        "dt = tree.DecisionTreeClassifier()\n",
        "dt.fit(X_train,y_train)\n",
        "importance = dt.feature_importances_\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiFUWWnE0QVc"
      },
      "source": [
        "def Feature_selector (X_train , y_train , tr=0.95):\n",
        "### YOUR CODE HERE ###\n",
        "  return ______"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTXjb1sWmDZL"
      },
      "source": [
        "## Part 4: Standardization [1 Marks]\n",
        "\n",
        "Standardizing the data usually means scaling our data to have a mean of zero and a standard deviation of one. \n",
        "\n",
        "**Note:** When we standardize a dataset, do we care if the data points are in our training set or test set? Yes! The training set is available for us to train a model - we can use it however we want. The test set, however, represents a subset of data that is not available for us during training. For example, the test set can represent the data that someone who bought our model would use to see how the model performs (which they are not willing to share with us).\n",
        "Therefore, we cannot compute the mean or standard deviation of the whole dataset to standardize it - we can only calculate the mean and standard deviation of the training set. However, when we sell a model to someone, we can say what our scalers (mean and standard deviation of our training set) was. They can scale their data (test set) with our training set's mean and standard deviation. Of course, there is no guarantee that the test set would have a mean of zero and a standard deviation of one, but it should work fine.\n",
        "\n",
        "**To summarize: We fit the StandardScaler only on the training set. We transform both training and test sets with that scaler.**\n",
        "\n",
        "1. Standardize the training  and test data ([Help](https://scikit-learn.org/stable/modules/preprocessing.html)) \n",
        "\n",
        "2. Call your ``Feature_selector`` function on the standardized training data with a threshold of 95\\%. \n",
        " * Plot the Cross validation accuracy when we have the standardized data (this part) and the original training data (last part) vs. the Number of features in a single plot (to compare them easily).\n",
        "\n",
        "3. Discuss how standardization (helped/hurt) your model and its performance? Discuss which cases lead to a higher cross validation accuracy (how many features? which features? What K?)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAwAGR6_2bu3"
      },
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 5: Decision Tree Classifier [1.5 Mark]\n",
        "\n",
        "Train a decision tree classifier on the standardized dataset (read the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) and check the example there.) Tune the `max_depth` and `min_samples_split` parameters of the tree using cross-validation (CV).\n",
        " * Compare the decision tree's performance (mean CV score) with KNN, both using all the features. \n"
      ],
      "metadata": {
        "id": "HdmuHF4kGH5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "laYRhTK4Kytl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7QjKjHn2TZR"
      },
      "source": [
        "## Part 6: Test Data [0.5 Mark]\n",
        "\n",
        "Now that you've created several models, pick your best one (highest CV accuracy) and apply it to the test dataset you had initially set aside. Discuss your results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acdJ7EijBYqB"
      },
      "source": [
        "### YOUR CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhCf82AOe3IS"
      },
      "source": [
        "References:\n",
        "\n",
        "https://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052\n",
        "\n",
        "https://www.analyticsvidhya.com/blog/2021/02/machine-learning-101-decision-tree-algorithm-for-classification/"
      ]
    }
  ]
}